# BanVic ETL Pipeline

Este projeto implementa um pipeline de **ETL (Extract, Transform, Load)** utilizando **Apache Airflow** e **PostgreSQL**.  
O objetivo Ã© centralizar dados de diferentes fontes.

---

# Estrutura do Projeto

```bash
.
â”œâ”€â”€ dags/                â†’ onde estÃ¡ sua DAG (`banvic_dag.py`)
â”‚   â””â”€â”€ banvic_dag.py
â”‚
â”œâ”€â”€ docker-compose.yml   â†’ para subir os containers
â”‚
â”œâ”€â”€ requirements.txt     â†’ para instalar dependÃªncias
â”‚
â”œâ”€â”€ README.md            â†’ com instruÃ§Ãµes
â”‚
â”œâ”€â”€ source_data/         â†’ dados de entrada em CSV necessÃ¡rios para reproduzir o pipeline
â”‚   â””â”€â”€ transacoes.csv
â”‚
â””â”€â”€ source_db/           â†’ dados de entrada em SQL necessÃ¡rios para reproduzir o pipeline
    â””â”€â”€ banvic.sql


## ðŸš€ VisÃ£o Geral do Pipeline

Fluxo de dados:

1. **ExtraÃ§Ã£o**  
   - Tabelas do banco de origem (`source_db`, inicializado a partir de `banvic.sql`)  
   - Arquivo CSV de transaÃ§Ãµes (`transacoes.csv`)  

2. **Armazenamento TemporÃ¡rio (Data Lake Local)**  
   - Os dados extraÃ­dos sÃ£o salvos em CSV no diretÃ³rio `data/`  
   - Estrutura de diretÃ³rios:  
     ```
     data/<ano>-<mes>-<dia>/
        â”œâ”€â”€ sql/
        â”‚    â”œâ”€â”€ clientes.csv
        â”‚    â”œâ”€â”€ contas.csv
        â”‚    â”œâ”€â”€ propostas_credito.csv
        â”‚    â””â”€â”€ agencias.csv
        â””â”€â”€ csv/
             â””â”€â”€ transacoes.csv
     ```

3. **Carga no Data Warehouse (DW)**  
   - Os CSVs sÃ£o carregados em um banco PostgreSQL separado (`dw_postgres`)  
   - As tabelas sÃ£o recriadas a cada execuÃ§Ã£o (idempotÃªncia)  

4. **VerificaÃ§Ã£o**  
   - O pipeline executa queries de contagem (`SELECT COUNT(*)`) no DW para garantir que os dados foram carregados corretamente.  

---

## ðŸ› ï¸ Tecnologias Utilizadas

- [Apache Airflow 2.7.2](https://airflow.apache.org/) â†’ OrquestraÃ§Ã£o das tarefas  
- [PostgreSQL 16](https://www.postgresql.org/) â†’ Banco de origem e Data Warehouse  
- [Docker Compose](https://docs.docker.com/compose/) â†’ Gerenciamento dos serviÃ§os  

---



ðŸ“Œ PrÃ©-requisitos Para Rodar o Projeto

Antes de rodar o pipeline, certifique-se de ter instalado:

 * Docker (>= 20.x)

 * Docker Compose (>= 1.29)

 * Git (para clonar o repositÃ³rio)

## Passo a Passo de ExecuÃ§Ã£o

1 Clonar o projeto

  git clone <url-do-repositorio>
  cd <nome-do-projeto>

2 Subir os containers
  docker compose up -d --build

  IrÃ¡ subir:
  * airflow_webserver â†’ Interface do Airflow
  * airflow_scheduler â†’ Scheduler do Airflow
  * airflow_db â†’ Banco interno do Airflow
  * source_db â†’ Banco de origem (com dados do banvic.sql)
  * dw_postgres â†’ Data Warehouse

3 Acessar o Airflow

Abra no navegador:
ðŸ‘‰ http://localhost:8080
UsuÃ¡rio e senha padrÃ£o:
  UsuÃ¡rio: admin
  Senha: admin

4 Executar a DAG

Na UI do Airflow, ative a DAG banvic_dag.
Clique em Trigger DAG para rodar manualmente.
Acompanhe os logs de cada task.

Para Conferir as Tabelas no DW (dw_postgres):

docker exec -it dw_postgres psql -U airflow -d airflow

E consultar por exemplo:

SELECT COUNT(*) FROM clientes;
SELECT COUNT(*) FROM transacoes;


ðŸ“Œ ObservaÃ§Ãµes

* O pipeline roda diariamente Ã s 04:35 da manhÃ£.
* As extraÃ§Ãµes sÃ£o idempotentes (recriam os arquivos a cada execuÃ§Ã£o).
* A DAG sÃ³ carrega dados no DW se todas as extraÃ§Ãµes forem concluÃ­das com sucesso.









